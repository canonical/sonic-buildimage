#!/usr/bin/env python3
'''
procdockerstatsd
Daemon which periodically gathers process and docker statistics and pushes the data to STATE_DB
'''

import os
import re
import subprocess
import sys
import time
import psutil
from datetime import datetime

from sonic_py_common import daemon_base
from swsscommon import swsscommon

VERSION = '1.0'

SYSLOG_IDENTIFIER = "procdockerstatsd"

REDIS_HOSTIP = "127.0.0.1"


class ProcDockerStats(daemon_base.DaemonBase):

    def __init__(self, log_identifier):
        super(ProcDockerStats, self).__init__(log_identifier)
        self.state_db = swsscommon.SonicV2Connector(host=REDIS_HOSTIP)
        self.state_db.connect("STATE_DB")

    def run_command(self, cmd):
        proc = subprocess.Popen(cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE)
        (stdout, stderr) = proc.communicate()
        if proc.returncode != 0:
            self.log_error("Error running command '{}'".format(cmd))
            return None
        else:
            return stdout

    def format_docker_cmd_output(self, cmdout):
        lines = cmdout.splitlines()
        keys = re.split("   +", lines[0])
        docker_data = dict()
        docker_data_list = []
        for line in lines[1:]:
            values = re.split("   +", line)
            docker_data = {key: value for key, value in zip(keys, values)}
            docker_data_list.append(docker_data)
        formatted_dict = self.create_docker_dict(docker_data_list)
        return formatted_dict

    def format_process_cmd_output(self, cmdout):
        lines = cmdout.splitlines()
        keys = re.split(" +", lines[0])
        key_list = [key for key in keys if key]
        process_data = dict()
        process_data_list = []
        for line in lines[1:]:
            values = re.split(" +", line)
            # To remove extra space before UID
            val_list = [val for val in values if val]
            # Merging extra columns created due to space in cmd ouput
            val_list[8:] = [' '.join(val_list[8:])]
            process_data = {key: value for key, value in zip(key_list, val_list)}
            process_data_list.append(process_data)
        return process_data_list

    def convert_to_bytes(self, value):
        UNITS_B = 'B'
        UNITS_KB = 'KB'
        UNITS_MB = 'MB'
        UNITS_MiB = 'MiB'
        UNITS_GiB = 'GiB'

        res = re.match(r'(\d+\.?\d*)([a-zA-Z]+)', value)
        value = float(res.groups()[0])
        units = res.groups()[1]
        if units.lower() == UNITS_KB.lower():
            value *= 1000
        elif units.lower() == UNITS_MB.lower():
            value *= (1000 * 1000)
        elif units.lower() == UNITS_MiB.lower():
            value *= (1024 * 1024)
        elif units.lower() == UNITS_GiB.lower():
            value *= (1024 * 1024 * 1024)

        return int(round(value))

    def create_docker_dict(self, dict_list):
        dockerdict = {}
        for row in dict_list[0:]:
            cid = row.get('CONTAINER ID')
            if cid:
                key = 'DOCKER_STATS|{}'.format(cid)
                dockerdict[key] = {}
                dockerdict[key]['NAME'] = row.get('NAME')

                cpu = row.get('CPU %').split("%")
                dockerdict[key]['CPU%'] = str(cpu[0])

                memuse = row.get('MEM USAGE / LIMIT').split(" / ")
                # converting MiB and GiB to bytes
                dockerdict[key]['MEM_BYTES'] = str(self.convert_to_bytes(memuse[0]))
                dockerdict[key]['MEM_LIMIT_BYTES'] = str(self.convert_to_bytes(memuse[1]))

                mem = row.get('MEM %').split("%")
                dockerdict[key]['MEM%'] = str(mem[0])

                netio = row.get('NET I/O').split(" / ")
                dockerdict[key]['NET_IN_BYTES'] = str(self.convert_to_bytes(netio[0]))
                dockerdict[key]['NET_OUT_BYTES'] = str(self.convert_to_bytes(netio[1]))

                blockio = row.get('BLOCK I/O').split(" / ")
                dockerdict[key]['BLOCK_IN_BYTES'] = str(self.convert_to_bytes(blockio[0]))
                dockerdict[key]['BLOCK_OUT_BYTES'] = str(self.convert_to_bytes(blockio[1]))

                dockerdict[key]['PIDS'] = row.get('PIDS')
        return dockerdict

    def update_dockerstats_command(self):
        cmd = "docker stats --no-stream -a"
        data = self.run_command(cmd)
        if not data:
            self.log_error("'{}' returned null output".format(cmd))
            return False
        dockerdata = self.format_docker_cmd_output(data)
        if not dockerdata:
            self.log_error("formatting for docker output failed")
            return False
        # wipe out all data from state_db before updating
        self.state_db.delete_all_by_pattern('STATE_DB', 'DOCKER_STATS|*')
        for k1, v1 in dockerdata.items():
            for k2, v2 in v1.items():
                self.update_state_db(k1, k2, v2)
        return True

    def get_proc_data(self, proc):
        result = {}
        result['UID'] = proc.uids().real
        result['PPID'] = proc.ppid()
        result['%CPU'] = round(proc.cpu_percent(), 1)
        result['%MEM'] = round(proc.memory_percent(), 1)
        result['CREATE_TIME'] = proc.create_time() # seconds
        stime = datetime.fromtimestamp(proc.create_time())
        uptime = time.time() - proc.create_time()
        stime_str = stime.strftime("%b%d") if uptime > (24 * 60 * 60) else stime.strftime("%H:%M")
        result['STIME'] = stime_str # Mmmdd or HH:MM
        result['ELAPSED'] = round(uptime, 2) # seconds
        tty = proc.terminal()
        result['TT'] = tty.lstrip('/dev/') if tty else "?"
        result['TIME'] = self.get_time_str(int(sum(proc.cpu_times()[:2]))) # [DD-]hh:mm:ss
        result['CPU_TIME_USER'] = proc.cpu_times().user
        result['CPU_TIME_SYSTEM'] = proc.cpu_times().system
        result['MEMORY_USAGE'] = proc.memory_info().rss # bytes
        result['CMD'] = " ".join(proc.cmdline()).strip(" \n")

        result = {key: str(result[key]) for key in result}
        return result

    def update_processstats_command(self):
        # wipe out all data before updating with new values
        self.state_db.delete_all_by_pattern('STATE_DB', 'PROCESS_STATS|*')
        for proc in psutil.process_iter():
            key = 'PROCESS_STATS|{}'.format(proc.pid)
            field_value = self.get_proc_data(proc)
            self.update_state_db_all(key,field_value)

    def get_time_str(self, time_second):
        days = int(time_second / (24 * 60 * 60))
        hours = int((time_second % (24 * 60 * 60)) / (60 * 60))
        minutes = int((time_second % (60 * 60)) / 60)
        seconds = int(time_second % 60)
        result = "{}{}{}{}".format(
            str(days).zfill(2) + "-" if days > 0 else "",
            str(hours).zfill(2) + ":",
            str(minutes).zfill(2) + ":",
            str(seconds).zfill(2)
        )
        return result

    def update_state_db(self, key1, key2, value2):
        self.state_db.set('STATE_DB', key1, key2, str(value2))

    def update_state_db_all(self, key1, dict1):
        self.state_db.hmset('STATE_DB', key1, dict1)

    def run(self):
        self.log_info("Starting up ...")

        if not os.getuid() == 0:
            self.log_error("Must be root to run this daemon")
            print("Must be root to run this daemon")
            sys.exit(1)

        while True:
            self.update_dockerstats_command()
            datetimeobj = datetime.now()
            # Adding key to store latest update time.
            self.update_state_db('DOCKER_STATS|LastUpdateTime', 'lastupdate', str(datetimeobj))
            self.update_processstats_command()
            self.update_state_db('PROCESS_STATS|LastUpdateTime', 'lastupdate', str(datetimeobj))

            # Data need to be updated every 2 mins. hence adding delay of 120 seconds
            time.sleep(120)

        self.log_info("Exiting ...")


def main():
    # Instantiate a ProcDockerStats object
    pd = ProcDockerStats(SYSLOG_IDENTIFIER)

    # Log all messages from INFO level and higher
    pd.set_min_log_priority_info()

    pd.run()


if __name__ == '__main__':
    main()
